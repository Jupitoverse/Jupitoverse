# ğŸ” ChatGPT API Integration & Token Management

> **How SR-Analyzer connects to ChatGPT via the AI Framework API**

This document explains how the Multi-Model RAG Pipeline authenticates and communicates with ChatGPT for LLM-powered SR analysis.

---

## ğŸ“¡ API Overview

### Endpoint
```
https://ai-framework1:8085/api/v1/call_llm
```

This is an **internal AI Framework gateway** that proxies requests to OpenAI's ChatGPT models. It handles authentication, rate limiting, and usage tracking.

### Default Model
```
gpt-4.1
```

---

## ğŸ”‘ Token Management

### Token Source

Tokens are stored in an Excel file:
```
sr-analyzer/semantic-resolution/tokens/Tokens.xlsx
```

**Required Excel Columns:**
| Column | Description |
|--------|-------------|
| `Email` or `Name` | User identifier for the token |
| `Token` | API authentication token |

### TokenManager Class

The `TokenManager` class (`multi_model_rag_pipeline_chatgpt.py`) handles:

1. **Loading tokens** from Excel on initialization
2. **Automatic rotation** when a token hits rate limits
3. **Tracking exhausted tokens** to avoid retry loops

```python
class TokenManager:
    def __init__(self, tokens_file: Path = None):
        # Default path: tokens/Tokens.xlsx
        self.tokens: List[Dict[str, str]] = []
        self.current_index = 0
        self.exhausted_tokens: set = set()
        self._load_tokens()
    
    def get_current_token(self) -> Optional[str]:
        # Returns active token, auto-rotating if exhausted
        
    def mark_exhausted(self) -> bool:
        # Marks current token as exhausted, rotates to next
        # Returns False if ALL tokens exhausted
```

---

## ğŸ“¤ API Request Format

### Request Headers

```python
headers = {
    "Content-Type": "application/json",
    "accept": "application/json",
    "API-Key": "<token>",                    # From TokenManager
    "X-Effective-Caller": "<email>"          # User email from Tokens.xlsx
}
```

### Request Payload

```python
payload = {
    "llm_model": "gpt-4.1",
    "messages": [
        {"role": "system", "content": "<system prompt>"},
        {"role": "user", "content": "<user prompt>"}
    ],
    "max_tokens": 8000
}
```

### Example cURL

```bash
curl -X POST "https://ai-framework1:8085/api/v1/call_llm" \
  -H "Content-Type: application/json" \
  -H "API-Key: YOUR_TOKEN_HERE" \
  -H "X-Effective-Caller: user@company.com" \
  -d '{
    "llm_model": "gpt-4.1",
    "messages": [
      {"role": "system", "content": "You are an expert analyst."},
      {"role": "user", "content": "Analyze this issue..."}
    ],
    "max_tokens": 8000
  }'
```

---

## ğŸ“¥ API Response Format

### Successful Response (200)

```json
{
    "message": "<LLM response text>",
    "input_tokens": 1500,
    "output_tokens": 800,
    "cost": 0.045,
    "finish_reason": "stop"
}
```

### Rate Limited Response (429)

When a token exceeds its quota:
- `TokenManager` marks it as exhausted
- Automatically rotates to the next available token
- Retries the request

---

## ğŸ”„ Token Rotation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Request Flow                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Get token from TokenManager                              â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  2. Make POST request to API                                 â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”œâ”€â”€ 200 OK â”€â”€â”€â”€â”€â”€â”€â–º Return response, track usage     â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”œâ”€â”€ 429 Rate Limit â”€â”€â–º Mark token exhausted          â”‚
â”‚         â”‚         â”‚                                          â”‚
â”‚         â”‚         â–¼                                          â”‚
â”‚         â”‚   More tokens available?                           â”‚
â”‚         â”‚         â”‚                                          â”‚
â”‚         â”‚    YES  â”‚   NO                                     â”‚
â”‚         â”‚    â–¼    â”‚   â–¼                                      â”‚
â”‚         â”‚  Rotate â”‚  Return error                            â”‚
â”‚         â”‚  & retryâ”‚  "All tokens exhausted"                  â”‚
â”‚         â”‚         â”‚                                          â”‚
â”‚         â””â”€â”€ Other Error â”€â”€â–º Return error response            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Usage Tracking

The `MultiModelLLM` class tracks usage across all calls:

```python
# Tracked metrics
self.total_calls = 0           # Number of LLM calls made
self.total_input_tokens = 0    # Total prompt tokens
self.total_output_tokens = 0   # Total completion tokens
self.total_cost = 0.0          # Total API cost (USD)

# Get usage summary
usage = llm.get_usage()
print(f"Calls: {usage['total_calls']}")
print(f"Tokens: {usage['total_input_tokens']} in / {usage['total_output_tokens']} out")
print(f"Cost: ${usage['total_cost']:.4f}")
print(f"Status: {usage['tokens_status']}")  # e.g., "Tokens: 3/5 available"
```

---

## âš™ï¸ Configuration

### Environment Variables

```bash
# Proxy bypass for internal API
NO_PROXY=ai-framework1
no_proxy=ai-framework1
```

### Command Line Arguments

```bash
python multi_model_rag_pipeline_chatgpt.py \
    --tokens /path/to/Tokens.xlsx \
    --model gpt-4.1
```

---

## ğŸ›¡ï¸ Error Handling

| Status Code | Meaning | Action |
|-------------|---------|--------|
| 200 | Success | Parse response, track usage |
| 429 | Rate Limited | Rotate token, retry |
| 500+ | Server Error | Log error, return failure |
| Timeout | No response in 300s | Return error |

---

## ğŸ“ File Locations

```
sr-analyzer/
â””â”€â”€ semantic-resolution/
    â”œâ”€â”€ tokens/
    â”‚   â””â”€â”€ Tokens.xlsx          # API tokens (Email, Token columns)
    â””â”€â”€ RAG/
        â””â”€â”€ pipeline/
            â””â”€â”€ multi_model_rag_pipeline_chatgpt.py  # TokenManager & MultiModelLLM
```

---

## ğŸ”— Related Components

- **TokenManager**: Token loading & rotation
- **MultiModelLLM**: LLM wrapper with retry logic
- **MultiModelSRPipeline**: Main pipeline using the LLM

---

*Part of SR-Analyzer Multi-Model RAG Pipeline*

