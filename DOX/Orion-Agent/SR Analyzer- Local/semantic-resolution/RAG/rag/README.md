# SR Analysis RAG Pipeline - DeepSeek Coder V2 Lite

## Overview

This RAG (Retrieval-Augmented Generation) pipeline analyzes service requests using DeepSeek Coder V2 Lite running locally for corporate/offline environments.

## Features

- ✅ **Offline Local LLM**: DeepSeek Coder V2 Lite (6.7B parameters)
- ✅ **Java Backend Analysis**: Intelligent Java error detection using javaMapping.db
- ✅ **Skills-Based Assignment**: Optimal team member assignment using people_skills.db
- ✅ **Semantic Workarounds**: Leverages pre-generated similar case solutions
- ✅ **AI-Enhanced Workarounds**: Combines semantic workarounds with intelligent analysis
- ✅ **Sequential Processing**: Processes each SR individually for accuracy
- ✅ **Corporate-Friendly**: No internet required after initial setup

## System Requirements

- **Python**: 3.8 or higher
- **RAM**: 16GB minimum (32GB recommended)
- **Disk Space**: ~15GB for model
- **GPU**: Optional but recommended (CUDA-compatible NVIDIA GPU)
- **OS**: Windows 10/11, Linux, or macOS

## Installation & Setup

### Step 1: Install Dependencies

```bash
cd "DTU MOD/rag"
pip install -r requirements.txt
```

Or use the batch file:
```bash
DOWNLOAD_MODEL.bat
```

### Step 2: Download DeepSeek Model

**IMPORTANT**: This step requires internet connectivity. Download once and use offline forever.

```bash
python download_deepseek_model.py
```

Or use:
```bash
DOWNLOAD_MODEL.bat
```

This will:
- Download DeepSeek Coder V2 Lite (~13GB)
- Cache it locally in `models/deepseek_coder_v2_lite/`
- Test the model to ensure it works
- Create offline-ready setup

**Download Time**: 10-30 minutes depending on internet speed

### Step 3: Prepare Input Files

1. Place your Excel file in: `DTU MOD/input/`
2. Ensure it has these columns:
   - **SR ID** (or Call ID)
   - **Priority** (P1/P2/P3/P4)
   - **Description** (issue description)
   - **Semantic Workaround** (pre-generated by extract_semantic_workarounds.py)
   - **Notes** (optional)

### Step 4: Run the Pipeline

```bash
python rag_pipeline.py
```

Or use:
```bash
RUN_RAG_PIPELINE.bat
```

## How It Works

### Pipeline Flow

```
1. Read Input Excel → DTU MOD/input/
2. Load DeepSeek Model (offline/local)
3. For each SR:
   ├─ Analyze Java backend (javaMapping.db)
   ├─ Detect Java failures and paths
   ├─ Determine optimal assignment (people_skills.db)
   ├─ Generate AI workaround (DeepSeek LLM)
   ├─ Create troubleshooting steps
   └─ Store result temporarily
4. Compile all results to Excel
5. Save to → DTU MOD/llm output/
```

### Intelligence Sources

1. **javaMapping.db**: Java backend code analysis
   - Detects Java exceptions and errors
   - Identifies specific Java class files
   - Provides file paths for debugging

2. **people_skills.db**: Team skills and expertise
   - Matches SR requirements with team capabilities
   - Optimizes assignment based on priority and complexity
   - Prioritizes Java experts for Java failures

3. **Semantic Workaround**: Pre-generated similar cases
   - Provides foundation for AI workaround generation
   - Contains proven solutions from historical data

4. **DeepSeek Coder V2 Lite**: Local LLM analysis
   - Enhances semantic workarounds with context
   - Generates step-by-step troubleshooting
   - Provides intelligent insights

## Output Format

The pipeline generates an Excel file with these columns:

| Column | Description |
|--------|-------------|
| SR ID | Service request identifier |
| Priority | Customer priority (P1/P2/P3/P4) |
| Assigned To | Optimal team member (skills-based) |
| Java Failure Detected | Yes/No - Java backend error detection |
| Java Failure Path | Complete Java class path (if detected) |
| Semantic Workaround | Pre-generated similar case solution |
| AI Workaround | Enhanced workaround from DeepSeek |
| Troubleshooting Steps | Step-by-step resolution guide |
| Original Notes | Original SR notes |
| Original Summary | Original SR description |

**Output Location**: `DTU MOD/llm output/<filename>_analysis_<timestamp>.xlsx`

## Corporate/Offline Usage

### After Initial Setup (One-Time)

Once the model is downloaded, **NO INTERNET is required**:

1. Model runs 100% locally
2. All databases are local
3. No external API calls
4. Corporate firewall/proxy friendly

### GPU vs CPU Performance

- **With GPU**: 2-5 seconds per SR
- **Without GPU**: 10-30 seconds per SR

## Troubleshooting

### Model Not Found Error

```
ERROR: DeepSeek model not found!
```

**Solution**: Run `DOWNLOAD_MODEL.bat` or `python download_deepseek_model.py`

### Out of Memory Error

```
CUDA out of memory
```

**Solution**: 
1. Close other GPU applications
2. Reduce batch size in code
3. Use CPU mode instead (slower but works)

### Excel File Not Found

```
No Excel files found in input directory
```

**Solution**: Place your Excel file in `DTU MOD/input/`

### Database Connection Error

```
Error querying Java database
```

**Solution**: Ensure `javaMapping.db` and `people_skills.db` exist in `vector store/`

## Performance Optimization

### For Faster Processing

1. **Use GPU**: Install CUDA and PyTorch with GPU support
2. **Quantization**: Model uses float16 for faster inference
3. **Batch Processing**: Pipeline processes SRs efficiently
4. **Local Cache**: Model loads once, used for all SRs

### Estimated Processing Time

- **10 SRs**: 1-2 minutes (GPU) or 5-10 minutes (CPU)
- **50 SRs**: 5-10 minutes (GPU) or 25-50 minutes (CPU)
- **100 SRs**: 10-20 minutes (GPU) or 50-100 minutes (CPU)

## Integration with RAG System

This pipeline is designed for RAG (Retrieval-Augmented Generation) integration:

1. **Input**: Excel with semantic workarounds (from historical retrieval)
2. **Processing**: DeepSeek LLM augments with intelligent analysis
3. **Output**: Enhanced analysis ready for downstream systems

## Advanced Configuration

### Modify Model Parameters

Edit `rag_pipeline.py`:

```python
# Change temperature for more/less creative responses
ai_response = self.analyzer.generate_response(
    analysis_prompt, 
    max_tokens=1024,  # Increase for longer responses
    temperature=0.3   # 0.0=deterministic, 1.0=creative
)
```

### Custom Assignment Logic

Edit `DatabaseHandler.get_best_assignee()` for custom team assignment rules.

### Java Detection Patterns

Edit `DatabaseHandler.query_java_mapping()` to add custom Java error patterns.

## File Structure

```
DTU MOD/
├── rag/
│   ├── rag_pipeline.py           # Main pipeline script
│   ├── download_deepseek_model.py # Model downloader
│   ├── requirements.txt           # Python dependencies
│   ├── DOWNLOAD_MODEL.bat         # Windows model downloader
│   ├── RUN_RAG_PIPELINE.bat       # Windows pipeline runner
│   ├── README.md                  # This file
│   └── models/
│       └── deepseek_coder_v2_lite/ # Model cache (after download)
├── input/                         # Place input Excel here
├── llm output/                    # Generated analysis output
└── Semantic workarounds/          # Pre-generated workarounds
```

## Support & Maintenance

### Regular Updates

1. Update Python packages: `pip install -r requirements.txt --upgrade`
2. Refresh databases: Ensure javaMapping.db and people_skills.db are current
3. Model updates: Re-download if newer version available

### Logs & Debugging

Check console output for detailed processing information:
- ✅ Success indicators
- ⚠️  Warnings
- ❌ Errors with troubleshooting hints

## FAQs

**Q: Can I use a different LLM?**  
A: Yes! Modify `DeepSeekAnalyzer` class to use any Hugging Face model.

**Q: Does this work on Mac/Linux?**  
A: Yes! Use `python rag_pipeline.py` directly. Batch files are Windows-only.

**Q: How accurate is Java detection?**  
A: Depends on javaMapping.db completeness. More data = better detection.

**Q: Can I run this on a laptop?**  
A: Yes, but 16GB+ RAM recommended. Works on CPU but slower.

**Q: Is internet required after setup?**  
A: No! Fully offline after model download.

## License

This pipeline is part of the SR Analysis System. Use responsibly in corporate environments.

---

**Created for**: Intelligent SR Analysis with Offline LLM  
**Model**: DeepSeek Coder V2 Lite (6.7B)  
**Version**: 1.0  
**Last Updated**: 2024

