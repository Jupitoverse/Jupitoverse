{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab99179a",
      "metadata": {},
      "source": [
        "### LangChain Intro — Local Ollama (Qwen)\n",
        "\n",
        "This notebook uses **Ollama** with a local model. **No API key or .env file is used or required.**\n",
        "\n",
        "**Prerequisites:** Ollama running locally and at least one model installed.  \n",
        "Run `ollama list` in a terminal to see installed models. Set `OLLAMA_MODEL` in the next cell to the **NAME** from that list (e.g. `qwen2.5-coder:14b-instruct-q8_0` or `llama3.2:latest`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "63f17654",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama agent created (no API key used). Invoke with: agent_ollama.invoke({'messages': [...]})\n"
          ]
        }
      ],
      "source": [
        "# No .env or API key needed for Ollama — it runs fully locally.\n",
        "from langchain.agents import create_agent\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Use the exact NAME from `ollama list` (e.g. qwen2.5-coder:14b-instruct-q8_0, llama3.2:latest)\n",
        "OLLAMA_MODEL = \"qwen2.5-coder:14b-instruct-q8_0\"\n",
        "ollama_llm = ChatOllama(model=OLLAMA_MODEL, temperature=0.7)\n",
        "\n",
        "agent_ollama = create_agent(\n",
        "    model=ollama_llm,\n",
        "    tools=[],\n",
        "    system_prompt=\"You are helpful.\",\n",
        ")\n",
        "print(\"Ollama agent created (no API key used). Invoke with: agent_ollama.invoke({'messages': [...]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "153b11be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello!\n"
          ]
        }
      ],
      "source": [
        "# Test the basic Ollama agent (use \"messages\" as the input key)\n",
        "try:\n",
        "    out = agent_ollama.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}]})\n",
        "    last = out.get(\"messages\", [])[-1]\n",
        "    print(last.content if hasattr(last, \"content\") and isinstance(last.content, str) else out)\n",
        "except Exception as e:\n",
        "    err = str(e).lower()\n",
        "    if \"not found\" in err or \"404\" in err:\n",
        "        print(\"Model not found. Run 'ollama list' and set OLLAMA_MODEL in the cell above to one of the NAME values.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c093bb",
      "metadata": {},
      "source": [
        "#### Agent with weather tool\n",
        "\n",
        "Same `get_weather` tool as in the Amdocs notebook; agent can use it for weather questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bda23e92",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the weather in a city.\"\"\"\n",
        "    return f\"The weather in {city} is sunny.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "55546ef4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama agent with weather tool created.\n"
          ]
        }
      ],
      "source": [
        "agent_ollama_weather = create_agent(\n",
        "    model=ollama_llm,\n",
        "    tools=[get_weather],\n",
        "    system_prompt=\"You are helpful. Use the get_weather tool when the user asks about weather in a city.\",\n",
        ")\n",
        "print(\"Ollama agent with weather tool created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63aaaf36",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\": \"get_weather\",\n",
            "  \"arguments\": {\n",
            "    \"city\": \"Mumbai\"\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Invoke with a weather question (input key must be \"messages\")\n",
        "try:\n",
        "    out = agent_ollama_weather.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in Mumbai?\"}]\n",
        "    })\n",
        "    last = out.get(\"messages\", [])[-1]\n",
        "    print(last.content if hasattr(last, \"content\") and isinstance(last.content, str) else out)\n",
        "except Exception as e:\n",
        "    if \"not found\" in str(e).lower():\n",
        "        print(\"Model not found. Set OLLAMA_MODEL in the first code cell to a name from 'ollama list'.\")\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
