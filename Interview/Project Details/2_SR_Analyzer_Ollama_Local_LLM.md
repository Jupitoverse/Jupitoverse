# SR Analyzer with Ollama (Local LLM) - Complete Project Documentation

## ğŸ“‹ Project Overview

**Project Name:** SR Analyzer - Ollama Edition  
**Type:** RAG (Retrieval-Augmented Generation) Pipeline  
**Tech Stack:** Python, Ollama (Qwen Model), ChromaDB, SQLite, Flask, Sentence Transformers  
**Purpose:** Intelligent Service Request analysis using locally-hosted LLM

---

## ğŸ¯ Project Description

### Short Description (30 seconds)
"This is a RAG-based SR analysis system that runs entirely on local hardware using Ollama with the Qwen model. It analyzes service requests by retrieving context from a ChromaDB vector database containing historical SRs and Java code, then generates intelligent workarounds using the local LLM. The system includes a Flask web interface for user feedback collection and admin batch processing."

### Detailed Description (2-3 minutes)
"I built this RAG pipeline to analyze telecom service requests using a locally-hosted LLM, eliminating the need for cloud API costs and ensuring data privacy.

**The RAG Architecture consists of:**

1. **Vector Database (ChromaDB):** Stores embeddings of historical SRs, Java code files, and workarounds. I used the all-MiniLM-L6-v2 sentence transformer model for generating 384-dimensional embeddings.

2. **Local LLM (Ollama + Qwen):** Instead of using expensive cloud APIs, I deployed Qwen model locally through Ollama. This provides:
   - Zero API costs
   - Data privacy (no data leaves the machine)
   - Offline capability
   - Customizable model parameters

3. **SQLite Databases:**
   - `javaMapping.db`: Maps Java classes to their file paths and error patterns
   - `people_skills.db`: Team member skills for intelligent SR assignment

4. **RAG Pipeline Flow:**
   - Input: Excel file with SRs
   - Step 1: Preprocess SR text (remove PII, normalize)
   - Step 2: Generate embedding for SR description
   - Step 3: Query ChromaDB for similar historical SRs
   - Step 4: Query javaMapping.db for Java error patterns
   - Step 5: Build context prompt with retrieved information
   - Step 6: Send to Qwen model for analysis
   - Step 7: Parse response and extract workaround
   - Step 8: Determine optimal assignee based on skills
   - Output: Excel with AI-generated workarounds and assignments

5. **Flask Web Interface:**
   - User portal for viewing SR analysis results
   - Admin portal for batch uploads and processing
   - Feedback collection for model improvement"

---

## ğŸ“ File Structure

```
Abhi-SR Analyzer- Ollama/
â”œâ”€â”€ semantic-resolution/
â”‚   â”œâ”€â”€ RAG/
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py           # Main RAG pipeline with DeepSeek
â”‚   â”‚   â”‚   â”œâ”€â”€ vectorstore_manager.py    # ChromaDB operations
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_generator.py    # Sentence transformer embeddings
â”‚   â”‚   â”‚   â””â”€â”€ prompt_templates.py       # LLM prompt templates
â”‚   â”‚   â”œâ”€â”€ input/                        # Input Excel files
â”‚   â”‚   â””â”€â”€ llm output/                   # Generated analysis results
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ sentence-transformers_all-MiniLM-L6-v2/  # Local embedding model
â”‚   â”‚
â”‚   â”œâ”€â”€ vector store/
â”‚   â”‚   â”œâ”€â”€ javaMapping.db                # Java class-to-path mapping
â”‚   â”‚   â”œâ”€â”€ people_skills.db              # Team skills database
â”‚   â”‚   â””â”€â”€ chroma_db/                    # ChromaDB vector store
â”‚   â”‚
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ feedback/
â”‚   â”‚       â”œâ”€â”€ user_portal.html          # User feedback interface
â”‚   â”‚       â”œâ”€â”€ admin_portal.html         # Admin batch processing
â”‚   â”‚       â””â”€â”€ sr_detail.html            # SR detail view
â”‚   â”‚
â”‚   â”œâ”€â”€ sr_feedback_app.py                # Flask application entry
â”‚   â”œâ”€â”€ batch_sr_analyser.py              # Batch processing script
â”‚   â”œâ”€â”€ comprehensive_sr_analyzer.py      # Full analysis pipeline
â”‚   â”œâ”€â”€ sr_text_preprocessor.py           # Text cleaning utilities
â”‚   â”œâ”€â”€ feedback_storage.py               # Feedback persistence
â”‚   â”œâ”€â”€ history_db_manager.py             # Historical data management
â”‚   â”‚
â”‚   â”œâ”€â”€ First_time.bat                    # Initial setup script
â”‚   â”œâ”€â”€ START_RAG_FEEDBACK_SYSTEM.bat     # Launch script
â”‚   â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ Technologies Used

| Category | Technology | Purpose |
|----------|------------|---------|
| LLM | Ollama + Qwen/DeepSeek | Local language model inference |
| Embeddings | Sentence Transformers (all-MiniLM-L6-v2) | Text to vector conversion |
| Vector DB | ChromaDB | Semantic similarity search |
| Database | SQLite | Java mapping & skills storage |
| Backend | Flask | Web interface |
| ML | PyTorch | Model inference |
| Data | Pandas, OpenPyXL | Excel processing |
| NLP | NLTK, spaCy | Text preprocessing |

---

## ğŸ”§ Key Technical Implementations

### 1. RAG Pipeline Architecture

```python
# RAG/rag/rag_pipeline.py
class SRAnalysisPipeline:
    def __init__(self):
        # Initialize components
        self.analyzer = DeepSeekAnalyzer(model_cache_dir)  # Local LLM
        self.db_handler = DatabaseHandler(java_db, skills_db)
        self.vectorstore = ChromaDBManager()
        
    def analyze_single_sr(self, sr_data: Dict) -> Dict:
        # Step 1: Query vector database for similar SRs
        similar_srs = self.vectorstore.similarity_search(
            sr_data['description'], 
            k=5
        )
        
        # Step 2: Check for Java errors
        java_analysis = self.db_handler.query_java_mapping(
            sr_data['description']
        )
        
        # Step 3: Build context-rich prompt
        prompt = self.build_prompt(sr_data, similar_srs, java_analysis)
        
        # Step 4: Generate analysis using local LLM
        ai_response = self.analyzer.generate_response(prompt)
        
        # Step 5: Determine assignment
        assignment = self.db_handler.get_best_assignee(
            sr_data['description'],
            sr_data['priority'],
            java_analysis['has_java_error']
        )
        
        return {
            'AI Workaround': ai_response,
            'Assigned To': assignment['assigned_to'],
            'Java Failure Detected': java_analysis['has_java_error']
        }
```

### 2. Local LLM Integration (Ollama)

```python
class DeepSeekAnalyzer:
    """Local LLM using Ollama"""
    
    def __init__(self, model_cache_dir: Path):
        self.model_name = "qwen:7b"  # or deepseek-coder
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_model(self):
        """Load model from local cache (offline mode)"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(self.cache_dir),
            local_files_only=True  # Force offline
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            str(self.cache_dir),
            torch_dtype=torch.float16,  # Half precision for speed
            device_map="auto",
            low_cpu_mem_usage=True,
            local_files_only=True
        )
    
    def generate_response(self, prompt: str) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.3,  # Lower = more deterministic
                do_sample=True,
                top_p=0.95
            )
        
        return self.tokenizer.decode(outputs[0])
```

### 3. ChromaDB Vector Store

```python
class VectorStoreManager:
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_or_create_collection(
            name="sr_history",
            metadata={"hnsw:space": "cosine"}  # Cosine similarity
        )
    
    def add_documents(self, documents: List[str], metadata: List[Dict]):
        embeddings = self.embedding_model.encode(documents)
        
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=documents,
            metadatas=metadata,
            ids=[str(uuid4()) for _ in documents]
        )
    
    def similarity_search(self, query: str, k: int = 5):
        query_embedding = self.embedding_model.encode([query])
        
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=k,
            include=["documents", "metadatas", "distances"]
        )
        
        return results
```

### 4. Java Error Detection

```python
def query_java_mapping(self, sr_description: str) -> Dict:
    """Query javaMapping.db for Java backend analysis"""
    conn = sqlite3.connect(self.java_db_path)
    
    # Java error patterns to detect
    java_patterns = [
        'Exception', 'Error', 'java.', 'NullPointer', 
        'SQLException', 'IOException', 'RuntimeException',
        'StackTrace', 'backend', 'service', 'controller'
    ]
    
    detected_patterns = []
    for pattern in java_patterns:
        if pattern.lower() in sr_description.lower():
            detected_patterns.append(pattern)
    
    # Search database for matching Java files
    if detected_patterns:
        cursor.execute("""
            SELECT file_path, class_name, error_type 
            FROM java_classes 
            WHERE error_type IN (?)
        """, (detected_patterns,))
        java_files = cursor.fetchall()
    
    return {
        'detected_patterns': detected_patterns,
        'java_files': java_files,
        'has_java_error': len(detected_patterns) > 0
    }
```

### 5. Embedding Generation

```python
from sentence_transformers import SentenceTransformer

class EmbeddingGenerator:
    def __init__(self):
        # Load model locally (no internet required)
        self.model = SentenceTransformer(
            './models/sentence-transformers_all-MiniLM-L6-v2'
        )
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """Generate 384-dimensional embedding"""
        # Preprocess text
        text = text.lower().strip()
        
        # Generate embedding
        embedding = self.model.encode(text, convert_to_numpy=True)
        
        return embedding  # Shape: (384,)
    
    def batch_generate(self, texts: List[str]) -> np.ndarray:
        """Batch processing for efficiency"""
        return self.model.encode(texts, batch_size=32, show_progress_bar=True)
```

---

## ğŸ”„ RAG Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SR Analysis RAG Pipeline                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Excel file with Service Requests                        â”‚
â”‚  - SR ID, Priority, Description, Notes                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Text Preprocessing                                      â”‚
â”‚  - Remove PII (emails, phone numbers)                           â”‚
â”‚  - Normalize whitespace                                         â”‚
â”‚  - Extract key terms                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Generate Embedding                                      â”‚
â”‚  - Use all-MiniLM-L6-v2 (384 dimensions)                        â”‚
â”‚  - Convert SR description to vector                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Retrieve Similar SRs (ChromaDB)                        â”‚
â”‚  - Cosine similarity search                                     â”‚
â”‚  - Return top 5 most similar historical SRs                     â”‚
â”‚  - Include their workarounds                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Java Error Detection (SQLite)                          â”‚
â”‚  - Query javaMapping.db                                         â”‚
â”‚  - Detect Java exception patterns                               â”‚
â”‚  - Find relevant Java class paths                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Build Context Prompt                                    â”‚
â”‚  - Current SR details                                           â”‚
â”‚  - Similar SR workarounds                                       â”‚
â”‚  - Java error context                                           â”‚
â”‚  - Resolution instructions                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: LLM Generation (Ollama/Qwen)                           â”‚
â”‚  - Send context-rich prompt                                     â”‚
â”‚  - Generate AI workaround                                       â”‚
â”‚  - Extract troubleshooting steps                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 7: Skill-Based Assignment                                  â”‚
â”‚  - Query people_skills.db                                       â”‚
â”‚  - Match SR type to team skills                                 â”‚
â”‚  - Consider priority and Java expertise                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT: Enhanced Excel file                                     â”‚
â”‚  - AI Workaround                                                â”‚
â”‚  - Assigned To                                                  â”‚
â”‚  - Java Failure Detected (Yes/No)                               â”‚
â”‚  - Java Failure Path                                            â”‚
â”‚  - Troubleshooting Steps                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Interview Questions & Answers

### Q1: What is RAG and why did you use it?
**Answer:** "RAG stands for Retrieval-Augmented Generation. It combines the power of retrieval systems (like vector databases) with generative AI. I used RAG because:
1. **Reduces hallucination:** The LLM generates responses based on retrieved factual context, not just its training data
2. **Domain-specific knowledge:** I can inject company-specific SR history and Java code context
3. **Up-to-date information:** The vector database can be updated with new SRs without retraining the model
4. **Cost-effective:** Smaller context windows are needed since relevant info is pre-filtered"

### Q2: Why did you choose Ollama with Qwen instead of OpenAI?
**Answer:** "Several reasons:
1. **Cost:** Zero API costs - runs entirely on local hardware
2. **Data Privacy:** Sensitive SR data never leaves our network
3. **Offline Capability:** Works without internet connection
4. **Latency:** No network round-trip to cloud APIs
5. **Customization:** Can fine-tune the model on our specific domain

The trade-off is that local models may be less capable than GPT-4, but for our structured SR analysis task, Qwen performs adequately."

### Q3: Explain how ChromaDB works and why you chose it.
**Answer:** "ChromaDB is an open-source vector database optimized for AI applications. I chose it because:
1. **Easy setup:** No external server needed, runs embedded in Python
2. **Persistent storage:** Data survives application restarts
3. **HNSW indexing:** Fast approximate nearest neighbor search
4. **Metadata filtering:** Can filter results by SR priority, date, etc.

It stores embeddings (384-dim vectors from sentence transformers) and allows cosine similarity search to find semantically similar SRs."

### Q4: How do you generate embeddings and what model did you use?
**Answer:** "I use the `all-MiniLM-L6-v2` model from Sentence Transformers. It:
- Generates 384-dimensional embeddings
- Is optimized for semantic similarity tasks
- Runs locally without API calls
- Processes text in ~10ms per sentence

The embedding captures semantic meaning, so 'server crashed' and 'backend failure' would have high similarity even though they share no words."

### Q5: How does the Java error detection work?
**Answer:** "I built a SQLite database (`javaMapping.db`) that maps:
- Java exception types to their common causes
- Java class names to file paths
- Error patterns to resolution steps

When analyzing an SR, I:
1. Scan the description for Java patterns (Exception, NullPointer, etc.)
2. Query the database for matching entries
3. Include relevant Java file paths in the LLM prompt
4. The LLM can then suggest specific code fixes"

### Q6: How do you handle the context window limitation of LLMs?
**Answer:** "LLMs have limited context windows (e.g., 4096 tokens). I handle this by:
1. **Selective retrieval:** Only fetch top 5 most similar SRs, not all
2. **Text truncation:** Limit each retrieved workaround to 500 characters
3. **Summarization:** Summarize long SR descriptions before embedding
4. **Priority ordering:** Put most relevant context first in the prompt
5. **Chunking:** For very long SRs, process in chunks and aggregate results"

### Q7: What's the difference between semantic search and keyword search?
**Answer:** "Keyword search matches exact terms - searching 'server error' won't find 'backend failure'. Semantic search uses embeddings to capture meaning:
- 'server error' and 'backend failure' have similar embeddings
- 'Java NullPointerException' matches 'null reference error'
- Works across different phrasings of the same issue

This is crucial for SR analysis where users describe the same problem differently."

### Q8: How do you evaluate the quality of generated workarounds?
**Answer:** "Multiple approaches:
1. **User feedback:** The Flask UI collects thumbs up/down and corrections
2. **Similarity scoring:** Compare generated workaround to historical successful ones
3. **Resolution rate:** Track if SRs with AI workarounds get resolved faster
4. **A/B testing:** Compare AI suggestions vs. manual workarounds
5. **Expert review:** Periodic audit by senior engineers"

---

## ğŸš€ How to Run

```bash
# 1. Install Ollama and pull model
ollama pull qwen:7b

# 2. Install dependencies
pip install -r requirements.txt

# 3. Initialize vector store (first time only)
python vectorstore_creation/create_vectorstore.py

# 4. Run the pipeline
python RAG/rag/rag_pipeline.py

# 5. Or start the web interface
python sr_feedback_app.py
# Access: http://localhost:5000
```

---

## ğŸ“Š Key Metrics

- **Embedding Dimension:** 384 (all-MiniLM-L6-v2)
- **Vector DB Size:** ~50,000 historical SRs
- **LLM Response Time:** 2-5 seconds (local GPU)
- **Similarity Search:** <100ms for top-5 results
- **Accuracy:** ~78% user satisfaction on generated workarounds
