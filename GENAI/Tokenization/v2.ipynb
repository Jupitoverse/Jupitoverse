{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7344b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/ag_news/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: 83c6f5fc-d943-48e6-b3ac-1ce85e517c72)')' thrown while requesting HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/ag_news/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: 37d2fec3-f6c0-4c35-8ca0-fc39187d3977)')' thrown while requesting HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/ag_news/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: e2635aef-706d-4da9-8871-d76631081303)')' thrown while requesting HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/ag_news/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: 267ac7e4-6cc0-406a-bcee-a5c9f2783ef8)')' thrown while requesting HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/ag_news/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: 30b25267-9d86-4b4f-8f55-baeb99f61971)')' thrown while requesting HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/ag_news/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)')))\"), '(Request ID: 1d5f9237-dbfd-42df-a111-c5e2a8432719)')' thrown while requesting HEAD https://huggingface.co/datasets/ag_news/resolve/main/README.md\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'ag_news' on the Hub (SSLError)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# ==============================================================\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Step 3. Load dataset\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ==============================================================\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mag_news\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain[:2\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m texts = [x[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), \u001b[33m\"\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhisha3\\Desktop\\Projects\\Learning\\Python\\venv\\Lib\\site-packages\\datasets\\load.py:1397\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1392\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1393\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1394\u001b[39m )\n\u001b[32m   1396\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhisha3\\Desktop\\Projects\\Learning\\Python\\venv\\Lib\\site-packages\\datasets\\load.py:1137\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1147\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhisha3\\Desktop\\Projects\\Learning\\Python\\venv\\Lib\\site-packages\\datasets\\load.py:1036\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1031\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1032\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1033\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1034\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1035\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhisha3\\Desktop\\Projects\\Learning\\Python\\venv\\Lib\\site-packages\\datasets\\load.py:972\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    960\u001b[39m     commit_hash = api.dataset_info(\n\u001b[32m    961\u001b[39m         path,\n\u001b[32m    962\u001b[39m         revision=revision,\n\u001b[32m    963\u001b[39m         timeout=\u001b[32m100.0\u001b[39m,\n\u001b[32m    964\u001b[39m     ).sha\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    966\u001b[39m     OfflineModeIsEnabled,\n\u001b[32m    967\u001b[39m     requests.exceptions.Timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m     httpx.TimeoutException,\n\u001b[32m    971\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    974\u001b[39m     message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is a gated dataset on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mConnectionError\u001b[39m: Couldn't reach 'ag_news' on the Hub (SSLError)"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# TOKENIZATION TECHNIQUES COMPARISON — BPE vs DCT vs EBT\n",
    "# ==============================================================\n",
    "\n",
    "# ✅ Step 1. Install dependencies\n",
    "!pip install datasets tokenizers sentence-transformers transformers tqdm matplotlib numpy scipy --quiet\n",
    "\n",
    "# ==============================================================\n",
    "# Step 2. Imports\n",
    "# ==============================================================\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# ==============================================================\n",
    "# Step 3. Load dataset\n",
    "# ==============================================================\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:2%]\")\n",
    "texts = [x[\"text\"] for x in dataset]\n",
    "print(\"Loaded\", len(texts), \"samples\")\n",
    "\n",
    "# ==============================================================\n",
    "# Step 4. Dynamic Contextual Tokenization (DCT)\n",
    "# ==============================================================\n",
    "print(\"\\nInitializing SentenceTransformer model for DCT...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def dynamic_contextual_tokenize(sentence, sim_threshold=0.55):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    vectors = model.encode(words, normalize_embeddings=True)\n",
    "    merged, i = [], 0\n",
    "    while i < len(words):\n",
    "        j = i\n",
    "        while j + 1 < len(words) and np.dot(vectors[j], vectors[j+1]) > sim_threshold:\n",
    "            j += 1\n",
    "        merged.append(\" \".join(words[i:j+1]))\n",
    "        i = j + 1\n",
    "    return merged\n",
    "\n",
    "# ==============================================================\n",
    "# Step 5. Entropy-Based Tokenization (EBT)\n",
    "# ==============================================================\n",
    "def shannon_entropy(seq):\n",
    "    counts = Counter(seq)\n",
    "    total = len(seq)\n",
    "    return -sum((c/total)*math.log2(c/total) for c in counts.values())\n",
    "\n",
    "def entropy_based_tokenize(text, window=4, threshold=0.8):\n",
    "    tokens, start = [], 0\n",
    "    for i in range(window, len(text)):\n",
    "        left = text[i-window:i]\n",
    "        right = text[i-window+1:i+1]\n",
    "        if abs(shannon_entropy(right) - shannon_entropy(left)) > threshold:\n",
    "            tokens.append(text[start:i])\n",
    "            start = i\n",
    "    tokens.append(text[start:])\n",
    "    tokens = [t.strip() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ==============================================================\n",
    "# Step 6. Baseline GPT-2 Tokenizer (BPE)\n",
    "# ==============================================================\n",
    "print(\"\\nLoading GPT-2 tokenizer (BPE baseline)...\")\n",
    "bpe_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def bpe_tokenize(text):\n",
    "    return bpe_tok.tokenize(text)\n",
    "\n",
    "# ==============================================================\n",
    "# Step 7. Test Sample Output\n",
    "# ==============================================================\n",
    "sample_text = \"Quantum physics explores atomic scale phenomena in the universe.\"\n",
    "print(\"\\nSample text:\", sample_text)\n",
    "print(\"DCT:\", dynamic_contextual_tokenize(sample_text))\n",
    "print(\"EBT:\", entropy_based_tokenize(sample_text))\n",
    "print(\"BPE:\", bpe_tokenize(sample_text))\n",
    "\n",
    "# ==============================================================\n",
    "# Step 8. Evaluation Function\n",
    "# ==============================================================\n",
    "def evaluate_tokenizer(fn, name, texts, n=100):\n",
    "    start = time.time()\n",
    "    avg_tokens = np.mean([len(fn(t)) for t in tqdm(texts[:n], desc=f\"Testing {name}\")])\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    return avg_tokens, elapsed\n",
    "\n",
    "# ==============================================================\n",
    "# Step 9. Run Evaluations\n",
    "# ==============================================================\n",
    "print(\"\\nEvaluating all tokenizers...\")\n",
    "\n",
    "results = {}\n",
    "for name, fn in {\n",
    "    \"DCT\": dynamic_contextual_tokenize,\n",
    "    \"EBT\": entropy_based_tokenize,\n",
    "    \"BPE\": bpe_tokenize\n",
    "}.items():\n",
    "    avg_tokens, elapsed = evaluate_tokenizer(fn, name, texts)\n",
    "    results[name] = (avg_tokens, elapsed)\n",
    "    print(f\"{name}: Avg Tokens = {avg_tokens:.2f}, Time = {elapsed:.2f}s\")\n",
    "\n",
    "# ==============================================================\n",
    "# Step 10. Plot Comparison Graphs\n",
    "# ==============================================================\n",
    "methods = list(results.keys())\n",
    "token_counts = [results[m][0] for m in methods]\n",
    "times = [results[m][1] for m in methods]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Plot 1 — Token Counts\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(methods, token_counts, color=['orange','green','skyblue'])\n",
    "plt.title(\"Average Tokens per Sentence\")\n",
    "plt.ylabel(\"Tokens (lower = more efficient)\")\n",
    "\n",
    "# Plot 2 — Runtime\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(methods, times, color=['orange','green','skyblue'])\n",
    "plt.title(\"Runtime (seconds per 100 samples)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "\n",
    "plt.suptitle(\"Comparison: BPE vs DCT vs EBT\", fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================\n",
    "# Step 11. Summary\n",
    "# ==============================================================\n",
    "print(\"\\n===== SUMMARY =====\")\n",
    "for m, (toks, t) in results.items():\n",
    "    print(f\"{m}: Avg Tokens = {toks:.2f}, Runtime = {t:.2f}s\")\n",
    "\n",
    "print(\"\\n✅ Done! You can now visually compare performance and efficiency of all tokenizers.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
